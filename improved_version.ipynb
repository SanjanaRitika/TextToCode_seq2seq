{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMon/SUnvDIeiGZmq5+ym2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaRitika/TextToCode_seq2seq/blob/main/improved_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veWk1KCnJ-gf",
        "outputId": "58b8a9d8-da2b-474b-8f4e-3d5e404a0a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "ds = load_dataset(\"Nan-Do/code-search-net-python\")\n",
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uzCCTCrMxqZ",
        "outputId": "66d6e44f-15f0-4088-b68e-9201dc3f462f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
            "        num_rows: 455243\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract relevant columns\n",
        "full_df = ds['train'].to_pandas()\n",
        "df = full_df[['code', 'code_tokens', 'docstring', 'docstring_tokens']]\n",
        "\n",
        "# Filter by sequence length constraints and sample\n",
        "length_filter = (df['docstring_tokens'].map(len) <= 50) & (df['code_tokens'].map(len) <= 80)\n",
        "filtered_data = df[length_filter]\n",
        "sampled_data = filtered_data.sample(n=10000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Sampled dataset shape: {sampled_data.shape}\")\n",
        "print(\"\\nFirst few examples:\")\n",
        "print(sampled_data.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caztwuc8M6mc",
        "outputId": "62ddbeac-4164-424b-f1f5-1ff6e8bd2afe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled dataset shape: (10000, 4)\n",
            "\n",
            "First few examples:\n",
            "                                                code  \\\n",
            "0  def _lib(self, name, only_if_have=False):\\n   ...   \n",
            "1  def open(self):\\n        \"\"\"Opens an existing ...   \n",
            "2  def hmac_sha1(self, key_handle, data, flags = ...   \n",
            "\n",
            "                                         code_tokens  \\\n",
            "0  [def, _lib, (, self, ,, name, ,, only_if_have,...   \n",
            "1  [def, open, (, self, ), :, try, :, self, ., gr...   \n",
            "2  [def, hmac_sha1, (, self, ,, key_handle, ,, da...   \n",
            "\n",
            "                                           docstring  \\\n",
            "0  Specify a linker library.\\n\\n        Example:\\...   \n",
            "1                           Opens an existing cache.   \n",
            "2  Have the YubiHSM generate a HMAC SHA1 of 'data...   \n",
            "\n",
            "                                    docstring_tokens  \n",
            "0                   [Specify, a, linker, library, .]  \n",
            "1                    [Opens, an, existing, cache, .]  \n",
            "2  [Have, the, YubiHSM, generate, a, HMAC, SHA1, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze token length statistics\n",
        "stats_df = pd.DataFrame({\n",
        "    'code_length': sampled_data['code_tokens'].apply(len),\n",
        "    'docstring_length': sampled_data['docstring_tokens'].apply(len)\n",
        "}).describe()\n",
        "\n",
        "print(\"Dataset Statistics:\")\n",
        "print(stats_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UKhenvdNAQc",
        "outputId": "7fe3302d-9ba5-4a94-bf93-6802aa52a03f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Statistics:\n",
            "        code_length  docstring_length\n",
            "count  10000.000000        10000.0000\n",
            "mean      48.047400           12.5361\n",
            "std       15.613723            8.8588\n",
            "min       20.000000            1.0000\n",
            "25%       35.000000            7.0000\n",
            "50%       46.000000           10.0000\n",
            "75%       60.000000           15.0000\n",
            "max       80.000000           50.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CodeBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\", add_prefix_space=True)\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "PAD_IDX = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Padding index: {PAD_IDX}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfLVaCAFNGaS",
        "outputId": "5e85bc66-790f-4a11-af51-7f8f639b92a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50265\n",
            "Padding index: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to IDs\n",
        "def tokenize_row(row):\n",
        "    code_toks = [str(t) for t in row['code_tokens']]\n",
        "    doc_toks = [str(t) for t in row['docstring_tokens']]\n",
        "\n",
        "    code_ids = tokenizer(code_toks, is_split_into_words=True,\n",
        "                         truncation=True, padding='max_length', max_length=128)['input_ids']\n",
        "    doc_ids = tokenizer(doc_toks, is_split_into_words=True,\n",
        "                        truncation=True, padding='max_length', max_length=64)['input_ids']\n",
        "\n",
        "    return pd.Series({'code_ids': code_ids, 'docstring_ids': doc_ids})\n",
        "\n",
        "tokenized_data = sampled_data.apply(tokenize_row, axis=1)\n",
        "dataset = pd.concat([sampled_data, tokenized_data], axis=1)\n",
        "print(\"\\nTokenization complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiwRMkODNL3T",
        "outputId": "6fc6216d-7876-4c67-f386-a3f506bf9210"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/validation/test sets\n",
        "train_data, temp_data = train_test_split(dataset, test_size=0.40, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.50, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_data)} | Validation: {len(val_data)} | Test: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEMKdSv1NS_S",
        "outputId": "bfed4823-13a6-4685-b7e0-8d4e056be8ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 6000 | Validation: 2000 | Test: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "def create_dataloader(df, batch_size=64, shuffle=True):\n",
        "    src_tensor = torch.tensor(df['code_ids'].tolist())\n",
        "    trg_tensor = torch.tensor(df['docstring_ids'].tolist())\n",
        "    dataset = TensorDataset(src_tensor, trg_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = create_dataloader(train_data)\n",
        "val_loader = create_dataloader(val_data)\n",
        "test_loader = create_dataloader(test_data, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFOh4r7iNdkf",
        "outputId": "1e58bdef-388c-4683-a44b-8fa15127f7c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, n_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.RNN(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            nonlinearity=\"tanh\",\n",
        "            batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0.0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [B, S]\n",
        "        embedded = self.dropout(self.embed(src))  # [B,S,E]\n",
        "        _, hidden = self.rnn(embedded)            # hidden: [L,B,H]\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class RNN_Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, n_layers=2, dropout=0.3, tie_weights=True):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.RNN(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            nonlinearity=\"tanh\",\n",
        "            batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0.0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # output projection\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # ✅ Weight tying (only if embed_dim == hidden_dim)\n",
        "        if tie_weights:\n",
        "            if embed_dim != hidden_dim:\n",
        "                print(\"⚠️ tie_weights skipped because embed_dim != hidden_dim\")\n",
        "            else:\n",
        "                self.out.weight = self.embed.weight  # tie\n",
        "\n",
        "    def forward(self, token, hidden):\n",
        "        # token: [B] (one token id per batch)  <-- simpler & standard\n",
        "        token = token.unsqueeze(1)  # [B,1]\n",
        "        embedded = self.dropout(self.embed(token))  # [B,1,E]\n",
        "        output, hidden = self.rnn(embedded, hidden) # output: [B,1,H]\n",
        "        prediction = self.out(output.squeeze(1))    # [B,V]\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class VanillaSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_ratio=0.5):\n",
        "        # trg: [B,T]\n",
        "        batch_size, trg_len = trg.shape\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        hidden = self.encoder(src)   # [L,B,H]\n",
        "\n",
        "        token = trg[:, 0]  # [B] (BOS)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            pred, hidden = self.decoder(token, hidden)\n",
        "            outputs[:, t] = pred\n",
        "\n",
        "            use_teacher = torch.rand(1).item() < teacher_ratio\n",
        "            token = trg[:, t] if use_teacher else pred.argmax(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "print(\"✅ Improved Vanilla RNN model defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfXwQQ7PN3Xh",
        "outputId": "aee82cf0-5d0a-4eb2-e00e-da7293052650"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Improved Vanilla RNN model defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Better hyperparameters (Model 1: Vanilla RNN)\n",
        "# ----------------------------\n",
        "EMBED_DIM  = 256     # make equal to HIDDEN_DIM for weight tying\n",
        "HIDDEN_DIM = 256\n",
        "N_LAYERS   = 2\n",
        "DROPOUT    = 0.3\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1.0\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Model 1 (improved encoder/decoder)\n",
        "# NOTE: this assumes you updated RNN_Encoder/RNN_Decoder to accept:\n",
        "# (vocab_size, embed_dim, hidden_dim, pad_idx, n_layers, dropout, tie_weights)\n",
        "# ----------------------------\n",
        "encoder1 = RNN_Encoder(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    pad_idx=PAD_IDX,\n",
        "    n_layers=N_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "decoder1 = RNN_Decoder(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    pad_idx=PAD_IDX,\n",
        "    n_layers=N_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    tie_weights=True   # works because EMBED_DIM == HIDDEN_DIM\n",
        ")\n",
        "\n",
        "model1 = VanillaSeq2Seq(encoder1, decoder1, DEVICE).to(DEVICE)\n",
        "\n",
        "# Multi-GPU (only if available)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "    model1 = nn.DataParallel(model1)\n",
        "\n",
        "# ----------------------------\n",
        "# Optimizer + Loss (better defaults)\n",
        "# ----------------------------\n",
        "optimizer1 = optim.AdamW(\n",
        "    model1.parameters(),\n",
        "    lr=3e-4,                 # smaller LR = more stable for seq2seq\n",
        "    betas=(0.9, 0.98),\n",
        "    weight_decay=1e-2\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
      ],
      "metadata": {
        "id": "zrps-oqTOLxm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "# Training function (improved)\n",
        "def train_model(model, loader, optimizer, criterion, clip, teacher_ratio=0.5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    for src, trg in loader:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
        "            output = model(src, trg, teacher_ratio)\n",
        "\n",
        "            # output: [B, T, V]\n",
        "            # ignore BOS position\n",
        "            output_flat = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            trg_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output_flat, trg_flat)\n",
        "\n",
        "        # backward (AMP-safe)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # clip gradients\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # token accuracy (ignore PAD)\n",
        "        preds = output[:, 1:].argmax(-1)              # [B, T-1]\n",
        "        gold  = trg[:, 1:]                           # [B, T-1]\n",
        "        mask  = gold != PAD_IDX\n",
        "        correct = ((preds == gold) & mask).sum().item()\n",
        "        total = mask.sum().item()\n",
        "        total_acc += correct / (total + 1e-9)\n",
        "\n",
        "    return total_loss / len(loader), total_acc / len(loader)\n",
        "\n",
        "\n",
        "# Evaluation function (improved)\n",
        "def eval_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "\n",
        "            with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
        "                output = model(src, trg, 0)  # No teacher forcing\n",
        "\n",
        "                output_flat = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "                trg_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "                loss = criterion(output_flat, trg_flat)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = output[:, 1:].argmax(-1)\n",
        "            gold  = trg[:, 1:]\n",
        "            mask  = gold != PAD_IDX\n",
        "            correct = ((preds == gold) & mask).sum().item()\n",
        "            total = mask.sum().item()\n",
        "            total_acc += correct / (total + 1e-9)\n",
        "\n",
        "    return total_loss / len(loader), total_acc / len(loader)\n",
        "\n",
        "print(\"✅ Improved training functions ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c0yz-w-OfGS",
        "outputId": "b7006978-e183-4e22-a970-11465f05da32"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Improved training functions ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2388924866.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss_m1 = float(\"inf\")\n",
        "print(\"Training Vanilla RNN Seq2Seq...\")\n",
        "\n",
        "train_losses_m1 = []\n",
        "val_losses_m1 = []\n",
        "train_accs_m1 = []\n",
        "val_accs_m1 = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # teacher forcing schedule\n",
        "    teacher_ratio = max(0.3, 0.7 - epoch * 0.05)\n",
        "\n",
        "    train_loss, train_acc = train_model(\n",
        "        model1,\n",
        "        train_loader,\n",
        "        optimizer1,\n",
        "        criterion,\n",
        "        CLIP,\n",
        "        teacher_ratio=teacher_ratio\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = eval_model(\n",
        "        model1,\n",
        "        val_loader,\n",
        "        criterion\n",
        "    )\n",
        "\n",
        "    train_losses_m1.append(train_loss)\n",
        "    val_losses_m1.append(val_loss)\n",
        "    train_accs_m1.append(train_acc)\n",
        "    val_accs_m1.append(val_acc)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    if val_loss < best_val_loss_m1:\n",
        "        best_val_loss_m1 = val_loss\n",
        "        torch.save(model1.state_dict(), \"vanilla_rnn.pt\")\n",
        "\n",
        "    print(\n",
        "        \"Epoch {:02d} | Time {:.0f}s | Train Loss {:.3f} | Val Loss {:.3f}\".format(\n",
        "            epoch + 1,\n",
        "            epoch_time,\n",
        "            train_loss,\n",
        "            val_loss\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(\"Vanilla RNN training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FpnzoCbOqWp",
        "outputId": "e1814d36-5696-40b7-9d17-77fdcb6f5b0f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Vanilla RNN Seq2Seq...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2388924866.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
            "/tmp/ipython-input-2388924866.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Time 72s | Train Loss 19.403 | Val Loss 12.681\n",
            "Epoch 02 | Time 70s | Train Loss 13.138 | Val Loss 10.003\n",
            "Epoch 03 | Time 68s | Train Loss 10.413 | Val Loss 8.846\n",
            "Epoch 04 | Time 68s | Train Loss 9.134 | Val Loss 8.468\n",
            "Epoch 05 | Time 68s | Train Loss 8.398 | Val Loss 8.029\n",
            "Epoch 06 | Time 68s | Train Loss 7.852 | Val Loss 7.693\n",
            "Epoch 07 | Time 68s | Train Loss 7.457 | Val Loss 7.473\n",
            "Epoch 08 | Time 68s | Train Loss 7.194 | Val Loss 7.235\n",
            "Epoch 09 | Time 68s | Train Loss 7.001 | Val Loss 7.135\n",
            "Epoch 10 | Time 68s | Train Loss 6.847 | Val Loss 7.041\n",
            "Vanilla RNN training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "model1.load_state_dict(torch.load('vanilla_rnn.pt'))\n",
        "test_loss = eval_model(model1, test_loader, criterion)\n",
        "print(f'Vanilla RNN Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
      ],
      "metadata": {
        "id": "1QscHjAwMYNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}