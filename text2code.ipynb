{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQJO/EJhuJ9XMVa+Qxw0GF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaRitika/TextToCode_seq2seq/blob/main/text2code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DvMy_uRFxKOB"
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets sacrebleu evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random, math, re, os, time\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "import sacrebleu\n"
      ],
      "metadata": {
        "id": "wCfY_3OQzr9b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGTSJ65q0PUt",
        "outputId": "b312a11d-6c73-42a8-c684-b5a90316f384"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_CANDIDATES = [\n",
        "    \"Nan-Do/code-search-net-python\",\n",
        "    \"code_search_net\",  # official HF name sometimes\n",
        "]\n",
        "\n",
        "ds = None\n",
        "last_err = None\n",
        "for name in DATASET_CANDIDATES:\n",
        "    try:\n",
        "        ds = load_dataset(name)\n",
        "        print(\"Loaded:\", name)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "\n",
        "if ds is None:\n",
        "    raise RuntimeError(f\"Could not load dataset from candidates. Last error: {last_err}\")\n",
        "\n",
        "ds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFjafjWx0bfF",
        "outputId": "f5170d66-a428-4a58-f9d2-8fadfb3a33d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: Nan-Do/code-search-net-python\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
              "        num_rows: 455243\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "print(ds[\"train\"].column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHpCNbY408oy",
        "outputId": "4fdcbb42-3642-4534-a078-23ec65f85b3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
            "        num_rows: 455243\n",
            "    })\n",
            "})\n",
            "['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pair(example):\n",
        "    # common patterns\n",
        "    for doc_key in [\"docstring\", \"func_documentation_string\", \"doc\"]:\n",
        "        if doc_key in example and example[doc_key]:\n",
        "            doc = example[doc_key]\n",
        "            break\n",
        "    else:\n",
        "        doc = \"\"\n",
        "\n",
        "    for code_key in [\"code\", \"func_code_string\", \"function\", \"content\"]:\n",
        "        if code_key in example and example[code_key]:\n",
        "            code = example[code_key]\n",
        "            break\n",
        "    else:\n",
        "        code = \"\"\n",
        "\n",
        "    # some datasets store dicts\n",
        "    if isinstance(doc, dict):\n",
        "        doc = doc.get(\"value\", \"\")\n",
        "    if isinstance(code, dict):\n",
        "        code = code.get(\"value\", \"\")\n",
        "\n",
        "    return doc, code\n",
        "\n",
        "# quick sanity check\n",
        "for i in range(3):\n",
        "    d, c = extract_pair(ds[\"train\"][i])\n",
        "    print(\"DOC:\", d[:120].replace(\"\\n\",\" \"))\n",
        "    print(\"CODE:\", c[:120].replace(\"\\n\",\" \"))\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeWEfMjq0_OO",
        "outputId": "5f05fccb-cde1-4364-d794-7136c55fd52b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DOC: Trains a k-nearest neighbors classifier for face recognition.      :param train_dir: directory that contains a sub-direc\n",
            "CODE: def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):     \"\"\"     Trains a \n",
            "---\n",
            "DOC: Recognizes faces in given image using a trained KNN classifier      :param X_img_path: path to image to be recognized   \n",
            "CODE: def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):     \"\"\"     Recognizes faces in given im\n",
            "---\n",
            "DOC: Shows the face recognition results visually.      :param img_path: path to image to be recognized     :param predictions\n",
            "CODE: def show_prediction_labels_on_image(img_path, predictions):     \"\"\"     Shows the face recognition results visually.    \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_N = 8000\n",
        "VAL_N   = 1000\n",
        "TEST_N  = 1000\n",
        "\n",
        "train_raw = ds[\"train\"].shuffle(seed=42).select(range(TRAIN_N))\n",
        "val_raw   = ds[\"validation\"].shuffle(seed=42).select(range(min(VAL_N, len(ds[\"validation\"]))))\n",
        "test_raw  = ds[\"test\"].shuffle(seed=42).select(range(min(TEST_N, len(ds[\"test\"]))))\n",
        "\n",
        "len(train_raw), len(val_raw), len(test_raw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "b2-6q3k_1C7B",
        "outputId": "b28565d1-a2d2-485c-af46-373cd78189a3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'validation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2066270016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mval_raw\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_raw\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedSplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             available_suggested_splits = [\n",
            "\u001b[0;31mKeyError\u001b[0m: 'validation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "def build_vocab(pairs, max_size=30000, min_freq=2):\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for doc, code in pairs:\n",
        "        cnt.update(tokenize(doc))\n",
        "        cnt.update(tokenize(code))\n",
        "    vocab = [PAD, BOS, EOS, UNK]\n",
        "    for tok, f in cnt.most_common():\n",
        "        if f < min_freq:\n",
        "            break\n",
        "        if tok in vocab:\n",
        "            continue\n",
        "        vocab.append(tok)\n",
        "        if len(vocab) >= max_size:\n",
        "            break\n",
        "    stoi = {t:i for i,t in enumerate(vocab)}\n",
        "    itos = {i:t for t,i in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "# Build vocab from training only (standard)\n",
        "train_pairs = [extract_pair(ex) for ex in train_raw]\n",
        "vocab, stoi, itos = build_vocab(train_pairs, max_size=30000, min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "vocab_size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODWq00yp1J4f",
        "outputId": "0fa6cfde-42a5-4a35-e18c-04386b819405"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_N = 8000\n",
        "VAL_N   = 1000\n",
        "TEST_N  = 1000\n",
        "\n",
        "train_raw = ds[\"train\"].shuffle(seed=42).select(range(min(TRAIN_N, len(ds[\"train\"]))))\n",
        "\n",
        "# ---- validation split handling ----\n",
        "if \"validation\" in ds:\n",
        "    val_raw = ds[\"validation\"].shuffle(seed=42).select(\n",
        "        range(min(VAL_N, len(ds[\"validation\"])))\n",
        "    )\n",
        "\n",
        "elif \"valid\" in ds:\n",
        "    val_raw = ds[\"valid\"].shuffle(seed=42).select(\n",
        "        range(min(VAL_N, len(ds[\"valid\"])))\n",
        "    )\n",
        "\n",
        "else:\n",
        "    # create validation from train (10%)\n",
        "    split_idx = int(0.9 * len(train_raw))\n",
        "    val_raw = train_raw.select(range(split_idx, len(train_raw)))\n",
        "    train_raw = train_raw.select(range(split_idx))\n",
        "    print(\"No validation split found — created from train set.\")\n",
        "\n",
        "# ---- test split handling ----\n",
        "if \"test\" in ds:\n",
        "    test_raw = ds[\"test\"].shuffle(seed=42).select(\n",
        "        range(min(TEST_N, len(ds[\"test\"])))\n",
        "    )\n",
        "else:\n",
        "    # fallback if test missing\n",
        "    test_raw = val_raw\n",
        "    print(\"No test split found — using validation as test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf6curtr1QwT",
        "outputId": "b8796044-d095-4956-eeba-f17ed29e25bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No validation split found — created from train set.\n",
            "No test split found — using validation as test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeGenDataset(Dataset):\n",
        "    def __init__(self, enc_list: List[EncodedExample]):\n",
        "        self.data = enc_list\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        return (\n",
        "            torch.tensor(ex.src_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_in_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_out_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.src_len, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_len, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(CodeGenDataset(train_enc), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(CodeGenDataset(val_enc), batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(CodeGenDataset(test_enc), batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "E1rn9uaf169P",
        "outputId": "be9130fa-dcc9-42ae-bea2-f35fd6803a9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_enc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2779503646.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodeGenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mval_loader\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodeGenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodeGenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_enc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "print(ds.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5bNmKsC2Wbm",
        "outputId": "9bfea006-dbbd-417d-dd4e-7833e8b605e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
            "        num_rows: 455243\n",
            "    })\n",
            "})\n",
            "dict_keys(['train'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_N = 8000\n",
        "VAL_N   = 1000\n",
        "TEST_N  = 1000\n",
        "\n",
        "train_raw = ds[\"train\"].shuffle(seed=42).select(range(min(TRAIN_N, len(ds[\"train\"]))))\n",
        "\n",
        "# ---- validation split handling ----\n",
        "if \"validation\" in ds:\n",
        "    val_raw = ds[\"validation\"].shuffle(seed=42).select(\n",
        "        range(min(VAL_N, len(ds[\"validation\"])))\n",
        "    )\n",
        "\n",
        "elif \"valid\" in ds:\n",
        "    val_raw = ds[\"valid\"].shuffle(seed=42).select(\n",
        "        range(min(VAL_N, len(ds[\"valid\"])))\n",
        "    )\n",
        "\n",
        "else:\n",
        "    # create validation from train (10%)\n",
        "    split_idx = int(0.9 * len(train_raw))\n",
        "    val_raw = train_raw.select(range(split_idx, len(train_raw)))\n",
        "    train_raw = train_raw.select(range(split_idx))\n",
        "    print(\"No validation split found — created from train set.\")\n",
        "\n",
        "# ---- test split handling ----\n",
        "if \"test\" in ds:\n",
        "    test_raw = ds[\"test\"].shuffle(seed=42).select(\n",
        "        range(min(TEST_N, len(ds[\"test\"])))\n",
        "    )\n",
        "else:\n",
        "    # fallback if test missing\n",
        "    test_raw = val_raw\n",
        "    print(\"No test split found — using validation as test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GZSdIhO2bVg",
        "outputId": "95188b70-9a3a-4813-e356-3cdc65f41f90"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No validation split found — created from train set.\n",
            "No test split found — using validation as test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "def build_vocab(pairs, max_size=30000, min_freq=2):\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for doc, code in pairs:\n",
        "        cnt.update(tokenize(doc))\n",
        "        cnt.update(tokenize(code))\n",
        "    vocab = [PAD, BOS, EOS, UNK]\n",
        "    for tok, f in cnt.most_common():\n",
        "        if f < min_freq:\n",
        "            break\n",
        "        if tok in vocab:\n",
        "            continue\n",
        "        vocab.append(tok)\n",
        "        if len(vocab) >= max_size:\n",
        "            break\n",
        "    stoi = {t:i for i,t in enumerate(vocab)}\n",
        "    itos = {i:t for t,i in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "# Build vocab from training only (standard)\n",
        "train_pairs = [extract_pair(ex) for ex in train_raw]\n",
        "vocab, stoi, itos = build_vocab(train_pairs, max_size=30000, min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "vocab_size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hDAjR8B2wjA",
        "outputId": "e4728391-f3ba-4e47-9a7b-767337fe4256"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SRC = 50\n",
        "MAX_TGT = 80\n",
        "\n",
        "def encode(tokens: List[str], max_len: int):\n",
        "    tokens = tokens[:max_len]\n",
        "    ids = [stoi.get(t, stoi[UNK]) for t in tokens]\n",
        "    return ids\n",
        "\n",
        "def add_bos_eos(ids: List[int], max_len: int):\n",
        "    ids = [stoi[BOS]] + ids + [stoi[EOS]]\n",
        "    # keep within max_len+2 budget\n",
        "    ids = ids[:max_len+2]\n",
        "    return ids\n",
        "\n",
        "def pad(ids: List[int], max_total_len: int):\n",
        "    if len(ids) < max_total_len:\n",
        "        ids = ids + [stoi[PAD]]*(max_total_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "@dataclass\n",
        "class EncodedExample:\n",
        "    src_ids: List[int]\n",
        "    tgt_in_ids: List[int]   # decoder input (starts with BOS)\n",
        "    tgt_out_ids: List[int]  # labels (shifted, ends with EOS)\n",
        "    src_len: int\n",
        "    tgt_len: int\n",
        "    raw_doc: str\n",
        "    raw_code: str\n",
        "\n",
        "def preprocess_pair(doc, code):\n",
        "    src_tok = tokenize(doc)\n",
        "    tgt_tok = tokenize(code)\n",
        "\n",
        "    src_ids = add_bos_eos(encode(src_tok, MAX_SRC), MAX_SRC)\n",
        "    tgt_ids = add_bos_eos(encode(tgt_tok, MAX_TGT), MAX_TGT)\n",
        "\n",
        "    # decoder: input is all but last, output is all but first\n",
        "    tgt_in  = tgt_ids[:-1]\n",
        "    tgt_out = tgt_ids[1:]\n",
        "\n",
        "    src_len = min(len(src_ids), MAX_SRC+2)\n",
        "    tgt_len = min(len(tgt_in), MAX_TGT+1)\n",
        "\n",
        "    src_ids = pad(src_ids, MAX_SRC+2)\n",
        "    tgt_in  = pad(tgt_in,  MAX_TGT+1)\n",
        "    tgt_out = pad(tgt_out, MAX_TGT+1)\n",
        "\n",
        "    return EncodedExample(src_ids, tgt_in, tgt_out, src_len, tgt_len, doc, code)\n",
        "\n",
        "train_enc = [preprocess_pair(*extract_pair(ex)) for ex in train_raw]\n",
        "val_enc   = [preprocess_pair(*extract_pair(ex)) for ex in val_raw]\n",
        "test_enc  = [preprocess_pair(*extract_pair(ex)) for ex in test_raw]\n",
        "\n",
        "len(train_enc), len(val_enc), len(test_enc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmwW31VL22h6",
        "outputId": "e426eceb-4557-49db-9cf3-8bf880134c5b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7200, 800, 800)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeGenDataset(Dataset):\n",
        "    def __init__(self, enc_list: List[EncodedExample]):\n",
        "        self.data = enc_list\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        return (\n",
        "            torch.tensor(ex.src_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_in_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_out_ids, dtype=torch.long),\n",
        "            torch.tensor(ex.src_len, dtype=torch.long),\n",
        "            torch.tensor(ex.tgt_len, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(CodeGenDataset(train_enc), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(CodeGenDataset(val_enc), batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(CodeGenDataset(test_enc), batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aBgQOjCC2_V0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_accuracy(logits, targets, pad_idx):\n",
        "    # logits: [B, T, V], targets: [B, T]\n",
        "    preds = logits.argmax(-1)\n",
        "    mask = targets != pad_idx\n",
        "    correct = (preds == targets) & mask\n",
        "    return correct.sum().item() / (mask.sum().item() + 1e-9)\n",
        "\n",
        "def ids_to_text(ids: List[int]):\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if i == stoi[EOS]:\n",
        "            break\n",
        "        if i in (stoi[PAD], stoi[BOS]):\n",
        "            continue\n",
        "        toks.append(itos.get(int(i), UNK))\n",
        "    # join with space; for code it won’t be perfect but works for BLEU + inspection\n",
        "    return \" \".join(toks)\n"
      ],
      "metadata": {
        "id": "2As-TIhd3EHa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.rnn = nn.RNN(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        # src: [B, S]\n",
        "        emb = self.emb(src)\n",
        "        out, h = self.rnn(emb)  # out: [B,S,H], h: [1,B,H]\n",
        "        return out, h\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.rnn = nn.RNN(emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc  = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in, h):\n",
        "        emb = self.emb(tgt_in)          # [B,T,E]\n",
        "        out, h = self.rnn(emb, h)       # [B,T,H]\n",
        "        logits = self.fc(out)           # [B,T,V]\n",
        "        return logits, h\n",
        "\n",
        "class Seq2SeqNoAttn(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, src_len, tgt_in):\n",
        "        _, h = self.encoder(src, src_len)\n",
        "        logits, _ = self.decoder(tgt_in, h)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "TmcNxyow3IZV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        emb = self.emb(src)\n",
        "        out, (h, c) = self.lstm(emb)  # h,c: [1,B,H]\n",
        "        return out, (h, c)\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc  = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in, hc):\n",
        "        emb = self.emb(tgt_in)\n",
        "        out, hc = self.lstm(emb, hc)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hc\n",
        "\n",
        "class Seq2SeqLSTMNoAttn(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, src_len, tgt_in):\n",
        "        _, hc = self.encoder(src, src_len)\n",
        "        logits, _ = self.decoder(tgt_in, hc)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "lckyyDlE3MCl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, attn_dim):\n",
        "        super().__init__()\n",
        "        self.W_enc = nn.Linear(enc_dim, attn_dim, bias=False)\n",
        "        self.W_dec = nn.Linear(dec_dim, attn_dim, bias=False)\n",
        "        self.v     = nn.Linear(attn_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, enc_out, dec_h, src_mask):\n",
        "        # enc_out: [B,S,enc_dim], dec_h: [B,dec_dim], mask: [B,S] (1 for valid)\n",
        "        # score = v^T tanh(W_enc(enc_out) + W_dec(dec_h))\n",
        "        e = self.W_enc(enc_out) + self.W_dec(dec_h).unsqueeze(1)  # [B,S,A]\n",
        "        scores = self.v(torch.tanh(e)).squeeze(-1)                # [B,S]\n",
        "\n",
        "        scores = scores.masked_fill(src_mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)                      # [B,S]\n",
        "        ctx = torch.bmm(attn.unsqueeze(1), enc_out).squeeze(1)    # [B,enc_dim]\n",
        "        return ctx, attn\n",
        "\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        emb = self.emb(src)\n",
        "        out, (h, c) = self.lstm(emb)  # out: [B,S,2H], h,c: [2,B,H]\n",
        "        return out, (h, c)\n",
        "\n",
        "class DecoderAttnLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, enc_out_dim, hid_dim, attn_dim=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n",
        "        self.attn = BahdanauAttention(enc_out_dim, hid_dim, attn_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim + enc_out_dim, hid_dim, batch_first=True)\n",
        "        self.fc   = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in, enc_out, hc, src_mask):\n",
        "        # teacher forcing full sequence\n",
        "        B, T = tgt_in.shape\n",
        "        emb = self.emb(tgt_in)  # [B,T,E]\n",
        "        h, c = hc\n",
        "        h = h.squeeze(0)        # [B,H]\n",
        "        c = c.squeeze(0)        # [B,H]\n",
        "\n",
        "        all_logits = []\n",
        "        all_attn = []\n",
        "\n",
        "        for t in range(T):\n",
        "            ctx, attn_w = self.attn(enc_out, h, src_mask)   # ctx: [B,enc_dim]\n",
        "            x = torch.cat([emb[:, t, :], ctx], dim=-1).unsqueeze(1)  # [B,1,E+enc]\n",
        "            out, (h1, c1) = self.lstm(x, (h.unsqueeze(0), c.unsqueeze(0)))\n",
        "            h = h1.squeeze(0); c = c1.squeeze(0)\n",
        "            logits = self.fc(out.squeeze(1))                # [B,V]\n",
        "            all_logits.append(logits.unsqueeze(1))\n",
        "            all_attn.append(attn_w.unsqueeze(1))\n",
        "\n",
        "        logits = torch.cat(all_logits, dim=1)  # [B,T,V]\n",
        "        attn   = torch.cat(all_attn, dim=1)    # [B,T,S]\n",
        "        return logits, attn\n",
        "\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "    def __init__(self, encoder_bi, decoder_attn, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_bi\n",
        "        self.decoder = decoder_attn\n",
        "        # bridge biLSTM states -> decoder initial\n",
        "        self.bridge_h = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
        "        self.bridge_c = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
        "\n",
        "    def forward(self, src, src_len, tgt_in):\n",
        "        enc_out, (h, c) = self.encoder(src, src_len)  # h,c: [2,B,H]\n",
        "        # concat directions\n",
        "        h_cat = torch.cat([h[0], h[1]], dim=-1)  # [B,2H]\n",
        "        c_cat = torch.cat([c[0], c[1]], dim=-1)\n",
        "        h0 = torch.tanh(self.bridge_h(h_cat)).unsqueeze(0)  # [1,B,H]\n",
        "        c0 = torch.tanh(self.bridge_c(c_cat)).unsqueeze(0)\n",
        "\n",
        "        src_mask = (src != stoi[PAD]).long()  # [B,S]\n",
        "        logits, attn = self.decoder(tgt_in, enc_out, (h0, c0), src_mask)\n",
        "        return logits, attn\n"
      ],
      "metadata": {
        "id": "NC4mnKlU3OeP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(model, loader, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    model.train() if is_train else model.eval()\n",
        "\n",
        "    ce = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out, src_len, tgt_len in loader:\n",
        "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            out = model(src, src_len, tgt_in)\n",
        "            if isinstance(out, tuple):\n",
        "                logits = out[0]\n",
        "            else:\n",
        "                logits = out\n",
        "\n",
        "            loss = ce(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "            acc  = token_accuracy(logits, tgt_out, stoi[PAD])\n",
        "\n",
        "            if is_train:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc  += acc\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / n_batches, total_acc / n_batches\n"
      ],
      "metadata": {
        "id": "ZCiAohf63STV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, epochs=8, lr=1e-3, ckpt_path=\"model.pt\"):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[]}\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer)\n",
        "        va_loss, va_acc = run_epoch(model, val_loader, None)\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"val_loss\"].append(va_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "\n",
        "        print(f\"Epoch {ep}: train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "\n",
        "        if va_loss < best_val:\n",
        "            best_val = va_loss\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            print(\"  saved:\", ckpt_path)\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "Ldzv--W03U1N"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256\n",
        "HID_DIM = 256\n",
        "\n",
        "# Model 1\n",
        "m1 = Seq2SeqNoAttn(\n",
        "    EncoderRNN(vocab_size, EMB_DIM, HID_DIM),\n",
        "    DecoderRNN(vocab_size, EMB_DIM, HID_DIM)\n",
        ")\n",
        "\n",
        "# Model 2\n",
        "m2 = Seq2SeqLSTMNoAttn(\n",
        "    EncoderLSTM(vocab_size, EMB_DIM, HID_DIM),\n",
        "    DecoderLSTM(vocab_size, EMB_DIM, HID_DIM)\n",
        ")\n",
        "\n",
        "# Model 3\n",
        "enc3 = EncoderBiLSTM(vocab_size, EMB_DIM, HID_DIM)\n",
        "# encoder out dim = 2*HID_DIM\n",
        "dec3 = DecoderAttnLSTM(vocab_size, EMB_DIM, enc_out_dim=2*HID_DIM, hid_dim=HID_DIM, attn_dim=256)\n",
        "m3 = Seq2SeqAttn(enc3, dec3, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM)\n",
        "\n",
        "hist1 = train_model(m1, epochs=8, ckpt_path=\"m1_rnn.pt\")\n",
        "hist2 = train_model(m2, epochs=8, ckpt_path=\"m2_lstm.pt\")\n",
        "hist3 = train_model(m3, epochs=8, ckpt_path=\"m3_attn.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UpVEFsYd3cV8",
        "outputId": "c848d3be-ec10-457a-873d-7b459434fd26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Seq2SeqNoAttn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2035741807.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Model 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m m1 = Seq2SeqNoAttn(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Seq2SeqNoAttn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCHJIuz0461V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}